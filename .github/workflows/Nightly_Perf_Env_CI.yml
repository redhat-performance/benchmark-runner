# Nightly CI https://github.com/marketplace/actions/deploy-nightly
# This is a nightly CI Pipeline against Performance environment
# GitHub Actions is limited 6 hours for each job
name: Deploy Perf Env Nightly CI

on:
  schedule:
    - cron: '0 4 * * 1-6' # run Monday through Saturday at 4 AM UTC/ 0 AM EDT

    
# Ensures that only one deploy task per branch/environment will run at a time.
concurrency:
  group: performance-environment

jobs:
  initialize_nightly:
    name: initialize perf nightly
    runs-on: ubuntu-latest
    outputs:
      start_time_output: ${{ steps.nightly_start_step.outputs.start_time }}
    steps:
    - uses: actions/checkout@v2
    - id: nightly_start_step
      run: echo "::set-output name=start_time::$(printf '%(%s)T' -1)"
    - name: ⚙️ Set SSH key
      env:
        PERF_PROVISION_PRIVATE_KEY: ${{ secrets.PERF_PROVISION_PRIVATE_KEY }}
        RUNNER_PATH: ${{ secrets.RUNNER_PATH }}
        PERF_PROVISION_IP: ${{ secrets.PERF_PROVISION_IP }}
        PERF_PROVISION_USER: ${{ secrets.PERF_PROVISION_USER }}
      run: |
        mkdir -p "$RUNNER_PATH/.ssh/"
        echo "$PERF_PROVISION_PRIVATE_KEY" > $RUNNER_PATH/private_key.txt
        sudo chmod 600 $RUNNER_PATH/private_key.txt
        echo "PERF_PROVISION_PRIVATE_KEY_PATH=$RUNNER_PATH/private_key.txt" >> "$GITHUB_ENV"
        cat >> "$RUNNER_PATH/.ssh/config" <<END
        Host provision
          HostName $PERF_PROVISION_IP
          User $PERF_PROVISION_USER
          IdentityFile $RUNNER_PATH/private_key.txt
          StrictHostKeyChecking no
          ServerAliveInterval 30
          ServerAliveCountMax 5
        END
    - name: ⚙ Set Kubeconfig
      env:
        KUBECONFIG: ${{ secrets.PERF_KUBECONFIG }}
        RUNNER_PATH: ${{ secrets.RUNNER_PATH }}
      run: |
        mkdir -p "$RUNNER_PATH/.kube/"
        echo "$KUBECONFIG" > "$RUNNER_PATH/.kube/config"
        echo "KUBECONFIG_PATH=$RUNNER_PATH/.kube/config" >> "$GITHUB_ENV"
    - name: ⚙ Copy config to provision
      env:
        CONTAINER_KUBECONFIG_PATH: ${{ secrets.CONTAINER_KUBECONFIG_PATH }}
        RUNNER_PATH: ${{ secrets.RUNNER_PATH }}
      run: |
        echo '>>>>>>>>>>>>>>>>>>>>>>>>>> Remove image on provision  >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>'
        echo "if [[ \"\$(sudo podman images -q quay.io/ebattat/benchmark-runner 2> /dev/null)\" != \"\" ]]; then sudo podman rmi -f \$(sudo podman images -q quay.io/ebattat/benchmark-runner 2> /dev/null); fi" > "$RUNNER_PATH/remove_image.sh"
        scp -r "$RUNNER_PATH/remove_image.sh" provision:"/tmp/remove_image.sh"
        ssh -t provision "chmod +x /tmp/remove_image.sh;/tmp/./remove_image.sh;rm -f /tmp/remove_image.sh"
        echo '>>>>>>>>>>>>>>>>>>>>>>>>>> Copy config to provision  >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>'
        scp -r "$RUNNER_PATH/.kube/config" provision:"$CONTAINER_KUBECONFIG_PATH"

  workload:
    name: workload
    runs-on: ubuntu-latest
    outputs:
      job_status: ${{ steps.job_step.outputs.status }}
    needs: initialize_nightly 
    strategy:
       # run one job every time
       max-parallel: 1
       # continue to next job if failed
       fail-fast: false
       matrix: 
          # full run: workload: [ 'stressng_pod', 'stressng_kata', 'stressng_vm', 'uperf_pod', 'uperf_kata', 'uperf_vm', 'hammerdb_pod_mariadb', 'hammerdb_kata_mariadb', 'hammerdb_vm_mariadb', 'hammerdb_pod_postgres', 'hammerdb_kata_postgres', 'hammerdb_vm_postgres', 'hammerdb_pod_mssql', 'hammerdb_kata_mssql', 'hammerdb_vm_mssql', 'vdbench_pod', 'vdbench_kata', 'vdbench_vm']
          workload: [ 'stressng_pod', 'stressng_kata', 'stressng_vm', 'uperf_pod', 'uperf_kata', 'uperf_vm', 'hammerdb_pod_mariadb', 'hammerdb_kata_mariadb', 'hammerdb_vm_mariadb', 'hammerdb_pod_postgres', 'hammerdb_kata_postgres', 'hammerdb_vm_postgres', 'hammerdb_pod_mssql', 'hammerdb_kata_mssql', 'hammerdb_vm_mssql']
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python 3.9
      uses: actions/setup-python@v2
      with:
        python-version: 3.9
    - name: Install latest benchmark-runner
      run: |
        python -m pip install --upgrade pip
        pip install benchmark-runner
    - name: ⚙️ Set SSH key
      env:
        PERF_PROVISION_PRIVATE_KEY: ${{ secrets.PERF_PROVISION_PRIVATE_KEY }}
        RUNNER_PATH: ${{ secrets.RUNNER_PATH }}
        PERF_PROVISION_IP: ${{ secrets.PERF_PROVISION_IP }}
        PERF_PROVISION_USER: ${{ secrets.PERF_PROVISION_USER }}
      run: |
        mkdir -p "$RUNNER_PATH/.ssh/"
        echo "$PERF_PROVISION_PRIVATE_KEY" > $RUNNER_PATH/private_key.txt
        sudo chmod 600 $RUNNER_PATH/private_key.txt
        echo "PERF_PROVISION_PRIVATE_KEY_PATH=$RUNNER_PATH/private_key.txt" >> "$GITHUB_ENV"
        cat >> "$RUNNER_PATH/.ssh/config" <<END
        Host provision
          HostName $PERF_PROVISION_IP
          User $PERF_PROVISION_USER
          IdentityFile $RUNNER_PATH/private_key.txt
          StrictHostKeyChecking no
          ServerAliveInterval 30
          ServerAliveCountMax 5
        END
    - name: ⚙ Set Kubeconfig
      env:
        KUBECONFIG: ${{ secrets.PERF_KUBECONFIG }}
        RUNNER_PATH: ${{ secrets.RUNNER_PATH }}
      run: |
          mkdir -p "$RUNNER_PATH/.kube/"
          echo "$KUBECONFIG" > "$RUNNER_PATH/.kube/config"
          echo "KUBECONFIG_PATH=$RUNNER_PATH/.kube/config" >> "$GITHUB_ENV"
    - name: ✔️ Run workload ${{ matrix.workload }}
      env:
        KUBEADMIN_PASSWORD: ${{ secrets.PERF_KUBEADMIN_PASSWORD }}
        PIN_NODE_BENCHMARK_OPERATOR: ${{ secrets.PERF_PIN_NODE_BENCHMARK_OPERATOR }}
        PIN_NODE1: ${{ secrets.PERF_PIN_NODE1 }}
        PIN_NODE2: ${{ secrets.PERF_PIN_NODE2 }}
        ELASTICSEARCH: ${{ secrets.PERF_ELASTICSEARCH }}
        ELASTICSEARCH_PORT: ${{ secrets.PERF_ELASTICSEARCH_PORT }}
        ELASTICSEARCH_USER: ${{ secrets.PERF_ELASTICSEARCH_USER }}
        ELASTICSEARCH_PASSWORD: ${{ secrets.PERF_ELASTICSEARCH_PASSWORD }}
        RUNNER_PATH: ${{ secrets.RUNNER_PATH }}
        CONTAINER_KUBECONFIG_PATH: ${{ secrets.CONTAINER_KUBECONFIG_PATH }}
        IBM_REGION_NAME: ${{ secrets.IBM_REGION_NAME }}
        IBM_ENDPOINT_URL: ${{ secrets.IBM_ENDPOINT_URL }}
        IBM_ACCESS_KEY_ID: ${{ secrets.IBM_ACCESS_KEY_ID }}
        IBM_SECRET_ACCESS_KEY: ${{ secrets.IBM_SECRET_ACCESS_KEY }}
        IBM_BUCKET: ${{ secrets.IBM_BUCKET }}
        IBM_KEY: ${{ secrets.IBM_KEY }}
        PERF_RUN_ARTIFACTS_URL: ${{ secrets.PERF_RUN_ARTIFACTS_URL }}
      run: |
        build=$(pip freeze | grep benchmark-runner | sed 's/==/=/g')
        build_version="$(cut -d'=' -f2 <<<"$build")"
        echo '>>>>>>>>>>>>>>>>>>>>>>>>>> Start E2E workload: ${{ matrix.workload }} >>>>>>>>>>>>>>>>>>>>>>>>>>'
        ssh -t provision "podman run --rm -t -e OCP_ENV_FLAVOR='PERF' -e WORKLOAD='${{ matrix.workload }}' -e KUBEADMIN_PASSWORD='$KUBEADMIN_PASSWORD' -e PIN_NODE_BENCHMARK_OPERATOR='$PIN_NODE_BENCHMARK_OPERATOR' -e PIN_NODE1='$PIN_NODE1' -e PIN_NODE2='$PIN_NODE2' -e ELASTICSEARCH='$ELASTICSEARCH' -e ELASTICSEARCH_PORT='$ELASTICSEARCH_PORT' -e ELASTICSEARCH_USER='$ELASTICSEARCH_USER' -e ELASTICSEARCH_PASSWORD='$ELASTICSEARCH_PASSWORD' -e IBM_REGION_NAME='$IBM_REGION_NAME' -e IBM_ENDPOINT_URL='$IBM_ENDPOINT_URL' -e IBM_ACCESS_KEY_ID='$IBM_ACCESS_KEY_ID' -e IBM_SECRET_ACCESS_KEY='$IBM_SECRET_ACCESS_KEY' -e IBM_BUCKET='$IBM_BUCKET' -e IBM_KEY='$IBM_KEY' -e PERF_RUN_ARTIFACTS_URL='$PERF_RUN_ARTIFACTS_URL' -e BUILD_VERSION='$build_version' -e RUN_TYPE='perf_ci' -e KATA_CPUOFFLINE_WORKAROUND='True' -e PERF_TIMEOUT='3600' -e log_level='INFO' -v '$CONTAINER_KUBECONFIG_PATH':'$CONTAINER_KUBECONFIG_PATH' --privileged 'quay.io/ebattat/benchmark-runner:v$build_version'"
        echo '>>>>>>>>>>>>>>>>>>>>>>>>>> End E2E workload: ${{ matrix.workload }} >>>>>>>>>>>>>>>>>>>>>>>>>>>>'
    - id: job_step
      if: always()
      run: |
          if [[ "${{ job.status }}" == "failure" || "${{ job.status }}" == "cancelled" ]]; then echo "::set-output name=status::${{ job.status }}"; fi
        
  finilize_nightly:
    name: finilize nightly
    runs-on: ubuntu-latest
    if: always()
    needs: [initialize_nightly, workload]
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python 3.9
      uses: actions/setup-python@v2
      with:
        python-version: 3.9
    - name: ⚙ Set START CI TIME
      run: echo "START_CI=${{ needs.initialize_nightly.outputs.start_time_output }}" >> "$GITHUB_ENV"
    - name: Install latest benchmark-runner
      run: |
        python -m pip install --upgrade pip
        pip install benchmark-runner
    - name: ⚙️ Set SSH key
      env:
        PERF_PROVISION_PRIVATE_KEY: ${{ secrets.PERF_PROVISION_PRIVATE_KEY }}
        RUNNER_PATH: ${{ secrets.RUNNER_PATH }}
        PERF_PROVISION_IP: ${{ secrets.PERF_PROVISION_IP }}
        PERF_PROVISION_USER: ${{ secrets.PERF_PROVISION_USER }}
      run: |
        mkdir -p "$RUNNER_PATH/.ssh/"
        echo "$PERF_PROVISION_PRIVATE_KEY" > $RUNNER_PATH/private_key.txt
        sudo chmod 600 $RUNNER_PATH/private_key.txt
        echo "PERF_PROVISION_PRIVATE_KEY_PATH=$RUNNER_PATH/private_key.txt" >> "$GITHUB_ENV"
        cat >> "$RUNNER_PATH/.ssh/config" <<END
        Host provision
          HostName $PERF_PROVISION_IP
          User $PERF_PROVISION_USER
          IdentityFile $RUNNER_PATH/private_key.txt
          StrictHostKeyChecking no
          ServerAliveInterval 30
          ServerAliveCountMax 5
        END
    - name: ⚙ Set Kubeconfig
      env:
        KUBECONFIG: ${{ secrets.PERF_KUBECONFIG }}
        RUNNER_PATH: ${{ secrets.RUNNER_PATH }}
      run: |
          mkdir -p "$RUNNER_PATH/.kube/"
          echo "$KUBECONFIG" > "$RUNNER_PATH/.kube/config"
          echo "KUBECONFIG_PATH=$RUNNER_PATH/.kube/config" >> "$GITHUB_ENV"
    - name: ✔️ Update status
      env:
        KUBEADMIN_PASSWORD: ${{ secrets.PERF_KUBEADMIN_PASSWORD }}
        PIN_NODE_BENCHMARK_OPERATOR: ${{ secrets.PERF_PIN_NODE_BENCHMARK_OPERATOR }}
        PIN_NODE1: ${{ secrets.PERF_PIN_NODE1 }}
        PIN_NODE2: ${{ secrets.PERF_PIN_NODE2 }}
        ELASTICSEARCH: ${{ secrets.PERF_ELASTICSEARCH }}
        ELASTICSEARCH_PORT: ${{ secrets.PERF_ELASTICSEARCH_PORT }}
        ELASTICSEARCH_USER: ${{ secrets.PERF_ELASTICSEARCH_USER }}
        ELASTICSEARCH_PASSWORD: ${{ secrets.PERF_ELASTICSEARCH_PASSWORD }}
        RUNNER_PATH: ${{ secrets.RUNNER_PATH }}
        CONTAINER_KUBECONFIG_PATH: ${{ secrets.CONTAINER_KUBECONFIG_PATH }}
      run: |
        # get repository last id
        declare -a repositories=('benchmark-operator' 'benchmark-wrapper')
        for repository in "${repositories[@]}"
        do
            git clone "https://github.com/cloud-bulldozer/$repository" "$RUNNER_PATH/$repository"
            pushd "$RUNNER_PATH/$repository"
            if [[ $repository == 'benchmark-operator' ]]
            then
                echo "BENCHMARK_OPERATOR_ID=$(git rev-parse @)" >> "$GITHUB_ENV"
                BENCHMARK_OPERATOR_ID=$(git rev-parse @)
            else
                echo "BENCHMARK_WRAPPER_ID=$(git rev-parse @)" >> "$GITHUB_ENV"
                BENCHMARK_WRAPPER_ID=$(git rev-parse @)
            fi
            popd
        done
        build=$(pip freeze | grep benchmark-runner | sed 's/==/=/g')
        build_version="$(cut -d'=' -f2 <<<"$build")"
        end=$(printf '%(%s)T' -1)
        ci_minutes_time=$(( (end - START_CI) / 60))
        # Check for workload failure or success => return pass/failed
        if [[ "${{needs.nightly.outputs.job_status}}" == "failure" || "${{needs.nightly.outputs.job_status}}" == "cancelled" ]]; then status="failed"; else status="pass"; fi
        echo '>>>>>>>>>>>>>>>>>>>>>>>>>> Update CI status $status >>>>>>>>>>>>>>>>>>>>>>>>>>'
        podman run --rm -e OCP_ENV_FLAVOR="PERF" -e KUBEADMIN_PASSWORD="$KUBEADMIN_PASSWORD" -e PIN_NODE_BENCHMARK_OPERATOR="$PIN_NODE_BENCHMARK_OPERATOR" -e PIN_NODE1="$PIN_NODE1" -e PIN_NODE2="$PIN_NODE2" -e ELASTICSEARCH="$ELASTICSEARCH" -e ELASTICSEARCH_PORT="$ELASTICSEARCH_PORT" -e ELASTICSEARCH_USER="$ELASTICSEARCH_USER" -e ELASTICSEARCH_PASSWORD="$ELASTICSEARCH_PASSWORD" -e BUILD_VERSION="$build_version" -e CI_STATUS="$status" -e CI_MINUTES_TIME="$ci_minutes_time" -e BENCHMARK_OPERATOR_ID="$BENCHMARK_OPERATOR_ID" -e BENCHMARK_WRAPPER_ID="$BENCHMARK_WRAPPER_ID" -e RUN_TYPE="perf_ci" -e PERF_TIMEOUT="3600" -e log_level="INFO" -v "$KUBECONFIG_PATH":"$CONTAINER_KUBECONFIG_PATH" --privileged "quay.io/ebattat/benchmark-runner:v$build_version"
        echo '>>>>>>>>>>>>>>>>>>>>>>>>>> Remove image on provision >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>'
        echo "if [[ \"\$(sudo podman images -q quay.io/ebattat/benchmark-runner 2> /dev/null)\" != \"\" ]]; then sudo podman rmi -f \$(sudo podman images -q quay.io/ebattat/benchmark-runner 2> /dev/null); fi" > "$RUNNER_PATH/remove_image.sh"
        scp -r "$RUNNER_PATH/remove_image.sh" provision:"/tmp/remove_image.sh"
        ssh -t provision "chmod +x /tmp/remove_image.sh;/tmp/./remove_image.sh;rm -f /tmp/remove_image.sh"
